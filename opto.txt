-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | General-purpose performant numeric optimization library
--   
--   Please see the README on Github at
--   <a>https://github.com/mstksg/opto#readme</a>
@package opto
@version 0.1.0.0


-- | Abstract over different types for mutable references of values.
module Numeric.Opto.Ref
class Monad m => Mutable m a where {
    type family Ref m a = (v :: Type) | v -> a;
    type Ref m a = MutVar (PrimState m) a;
}
thawRef :: Mutable m a => a -> m (Ref m a)
freezeRef :: Mutable m a => Ref m a -> m a
copyRef :: Mutable m a => Ref m a -> a -> m ()

-- | Apply a pure function on an immutable value onto a value stored in a
--   mutable reference.
modifyRef :: Mutable m a => Ref m a -> (a -> a) -> m ()

-- | <a>modifyRef</a>, but forces the result before storing it back in the
--   reference.
modifyRef' :: Mutable m a => Ref m a -> (a -> a) -> m ()

-- | Apply a pure function on an immutable value onto a value stored in a
--   mutable reference, returning a result value from that function.
updateRef :: Mutable m a => Ref m a -> (a -> (a, b)) -> m b

-- | <a>updateRef</a>, but forces the updated value before storing it back
--   in the reference.
updateRef' :: Mutable m a => Ref m a -> (a -> (a, b)) -> m b
thawRef :: (Mutable m a, Ref m a ~ MutVar (PrimState m) a, PrimMonad m) => a -> m (Ref m a)
freezeRef :: (Mutable m a, Ref m a ~ MutVar (PrimState m) a, PrimMonad m) => Ref m a -> m a
copyRef :: (Mutable m a, Ref m a ~ MutVar (PrimState m) a, PrimMonad m) => Ref m a -> a -> m ()

-- | Newtype wrapper that can provide any type with a <a>Mutable</a>
--   instance. Can be useful for avoiding orphan instances.
newtype MutRef a
MutRef :: a -> MutRef a
[runMutRef] :: MutRef a -> a
class Monad m => GMutable m f
newtype GRef m a
GRef :: GRef_ m (Rep a) () -> GRef m a
[unGRef] :: GRef m a -> GRef_ m (Rep a) ()
gThawRef :: (Generic a, GMutable m (Rep a)) => a -> m (GRef m a)
gFreezeRef :: (Generic a, GMutable m (Rep a)) => GRef m a -> m a
gCopyRef :: (Generic a, GMutable m (Rep a)) => GRef m a -> a -> m ()

-- | If you can provice a natural transformation from <tt>m</tt> to
--   <tt>n</tt>, you should be able to use a value as if it had
--   <tt><a>Mutable</a> n a</tt> if you have <tt><a>Mutable</a> m a</tt>.
reMutable :: forall m n a r. (Mutable m a, Monad n) => (forall x. m x -> n x) -> (Mutable n a => r) -> r

-- | If you can provice a natural transformation from <tt>m</tt> to
--   <tt>n</tt>, then <tt><a>Mutable</a> m a</tt> should also imply
--   <tt><a>Mutable</a> n a</tt>.
reMutableConstraint :: forall m n a. (Mutable m a, Monad n) => (forall x. m x -> n x) -> Mutable m a :- Mutable n a
newtype ReMutable (s :: Type) m a
ReMutable :: a -> ReMutable m a
newtype ReMutableTrans m n
RMT :: (forall x. m x -> n x) -> ReMutableTrans m n
[runRMT] :: ReMutableTrans m n -> forall x. m x -> n x
instance (GHC.Base.Monad n, Numeric.Opto.Ref.Mutable m a, Data.Reflection.Reifies s (Numeric.Opto.Ref.ReMutableTrans m n)) => Numeric.Opto.Ref.Mutable n (Numeric.Opto.Ref.ReMutable s m a)
instance Numeric.Opto.Ref.Mutable m c => Numeric.Opto.Ref.GMutable m (GHC.Generics.K1 i c)
instance (Numeric.Opto.Ref.GMutable m f, Numeric.Opto.Ref.GMutable m g) => Numeric.Opto.Ref.GMutable m (f GHC.Generics.:*: g)
instance Numeric.Opto.Ref.GMutable m f => Numeric.Opto.Ref.GMutable m (GHC.Generics.M1 i c f)
instance (Numeric.Opto.Ref.GMutable m f, Numeric.Opto.Ref.GMutable m g, Control.Monad.Primitive.PrimMonad m) => Numeric.Opto.Ref.GMutable m (f GHC.Generics.:+: g)
instance Control.Monad.Primitive.PrimMonad m => Numeric.Opto.Ref.Mutable m (Numeric.Opto.Ref.MutRef a)
instance Data.Vinyl.XRec.IsoHKD Numeric.Opto.Ref.MutRef a
instance Control.Monad.Primitive.PrimMonad m => Numeric.Opto.Ref.Mutable m GHC.Types.Int
instance Control.Monad.Primitive.PrimMonad m => Numeric.Opto.Ref.Mutable m GHC.Integer.Type.Integer
instance Control.Monad.Primitive.PrimMonad m => Numeric.Opto.Ref.Mutable m (GHC.Real.Ratio a)
instance Control.Monad.Primitive.PrimMonad m => Numeric.Opto.Ref.Mutable m GHC.Types.Float
instance Control.Monad.Primitive.PrimMonad m => Numeric.Opto.Ref.Mutable m GHC.Types.Double
instance Control.Monad.Primitive.PrimMonad m => Numeric.Opto.Ref.Mutable m (Data.Complex.Complex a)
instance Control.Monad.Primitive.PrimMonad m => Numeric.Opto.Ref.Mutable m (Data.Vector.Vector a)
instance (Control.Monad.Primitive.PrimMonad m, Data.Vector.Generic.Base.Vector v a) => Numeric.Opto.Ref.Mutable m (Data.Vector.Generic.Sized.Internal.Vector v n a)
instance (Control.Monad.Primitive.PrimMonad m, GHC.TypeNats.KnownNat n) => Numeric.Opto.Ref.Mutable m (Internal.Static.R n)
instance (Control.Monad.Primitive.PrimMonad m, GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat k) => Numeric.Opto.Ref.Mutable m (Internal.Static.L n k)
instance GHC.Base.Monad m => Numeric.Opto.Ref.Mutable m ()
instance (GHC.Base.Monad m, Numeric.Opto.Ref.Mutable m a, Numeric.Opto.Ref.Mutable m b) => Numeric.Opto.Ref.Mutable m (a, b)
instance (GHC.Base.Monad m, Numeric.Opto.Ref.Mutable m a, Numeric.Opto.Ref.Mutable m b, Numeric.Opto.Ref.Mutable m c) => Numeric.Opto.Ref.Mutable m (a, b, c)
instance (GHC.Base.Monad m, Numeric.Opto.Ref.Mutable m a, Numeric.Opto.Ref.Mutable m b, Numeric.Opto.Ref.Mutable m c, Numeric.Opto.Ref.Mutable m d) => Numeric.Opto.Ref.Mutable m (a, b, c, d)


-- | Conduits that are useful for sampling and running optimizers.
module Numeric.Opto.Run.Conduit

-- | Outputs a shuffled version of the input stream. Keeps entire input
--   stream in memory.
--   
--   NOTE: Pulls the entire input stream into memory first before
--   outputting anything.
shuffling :: PrimMonad m => Gen (PrimState m) -> ConduitT a a m ()

-- | Takes the first N items out of the input stream, shuffles them
--   in-memory, and outputs the shuffled result.
--   
--   Leaves the rest of the items in the stream.
--   
--   Use <a>forever</a> to repeat multiple times until the stream is
--   exhausted.
shufflingN :: PrimMonad m => Int -> Gen (PrimState m) -> ConduitT a a m ()

-- | Process an entire stream, and keep N random and shuffled items from
--   that stream. Is O(N) memory.
sinkSampleReservoir :: forall m v a o. (PrimMonad m, Vector v a) => Int -> Gen (PrimState m) -> ConduitT a o m (v a)

-- | Process an entire stream, and yield N random items from that stream.
--   Is O(N) memory.
--   
--   NOTE: Exhausts the entire input stream first before outputting
--   anything, but never keeps the entire original stream in memory.
samplingN :: PrimMonad m => Int -> Gen (PrimState m) -> ConduitT a a m ()

-- | Drops and lets items through randomly with a given probability.
skipSampling :: PrimMonad m => Double -> Gen (PrimState m) -> ConduitT a a m ()


-- | A unified interface for values in vector spaces that can be added and
--   scaled (purely and also in-place), and measured.
module Numeric.Opto.Update

-- | If <tt>a</tt> is an instance of <tt><a>Linear</a> c</tt>, you can
--   <i>add</i> together values of <tt>a</tt>, and <i>scale</i> them using
--   <tt>c</tt>s.
--   
--   For example, if you have a vector of doubles, you can add them
--   together component-wise, and scale them by multiplying every item by
--   the scalar.
--   
--   Mathematically, this means that <tt>a</tt> forms something like a
--   module or vector space over <tt>c</tt>, where <tt>c</tt> can be any
--   <a>Num</a> instance.
class Num c => Linear c a | a -> c

-- | Add together <tt>a</tt>s. Should be associative.
--   
--   <pre>
--   x .+. (y .+. z) == (x .+. y) .+. z
--   </pre>
--   
--   If <tt>a</tt> is an instance of <a>Num</a>, this can be just <a>+</a>.
(.+.) :: Linear c a => a -> a -> a

-- | The "zero" <tt>a</tt>, meant to form an identity with <a>.+.</a>.
--   
--   <pre>
--   x .+. zeroL == x
--   zeroL .+. y == y
--   </pre>
--   
--   If <tt>a</tt> is an instance of <a>Num</a>, this can be just 0.
zeroL :: Linear c a => a

-- | Scale an <tt>a</tt> by a factor <tt>c</tt>. Should distribute over
--   <a>.+.</a>.
--   
--   <pre>
--   a .* (x .+. y) == (a .* x) .+. (a .* y)
--   a .* (b .* c)  == (a * b) .* c
--   </pre>
(.*) :: Linear c a => c -> a -> a

-- | Add together <tt>a</tt>s. Should be associative.
--   
--   <pre>
--   x .+. (y .+. z) == (x .+. y) .+. z
--   </pre>
--   
--   If <tt>a</tt> is an instance of <a>Num</a>, this can be just <a>+</a>.
(.+.) :: (Linear c a, ADTRecord a, Constraints a (Linear c)) => a -> a -> a

-- | The "zero" <tt>a</tt>, meant to form an identity with <a>.+.</a>.
--   
--   <pre>
--   x .+. zeroL == x
--   zeroL .+. y == y
--   </pre>
--   
--   If <tt>a</tt> is an instance of <a>Num</a>, this can be just 0.
zeroL :: (Linear c a, ADTRecord a, Constraints a (Linear c)) => a

-- | Scale an <tt>a</tt> by a factor <tt>c</tt>. Should distribute over
--   <a>.+.</a>.
--   
--   <pre>
--   a .* (x .+. y) == (a .* x) .+. (a .* y)
--   a .* (b .* c)  == (a * b) .* c
--   </pre>
(.*) :: (Linear c a, ADTRecord a, Constraints a (Linear c)) => c -> a -> a
infixl 6 .+.
infixl 7 .*

-- | Sum over a <a>Foldable</a> container of <tt><a>Linear</a> c a</tt>
sumLinear :: (Linear c a, Foldable t) => t a -> a

-- | An implementation of <a>.+.</a> that works for records where every
--   field is an instance of <tt><a>Linear</a> c</tt> (that is, every field
--   is additive and can be scaled by the same <tt>c</tt>).
gAdd :: forall c a. (ADTRecord a, Constraints a (Linear c)) => a -> a -> a

-- | An implementation of <a>zeroL</a> that works for records where every
--   field is an instance of <tt><a>Linear</a> c</tt> (that is, every field
--   is additive and can be scaled by the same <tt>c</tt>).
gZeroL :: forall c a. (ADTRecord a, Constraints a (Linear c)) => a

-- | An implementation of <a>.*</a> that works for records where every
--   field is an instance of <tt><a>Linear</a> c</tt> (that is, every field
--   is additive and can be scaled by the same <tt>c</tt>).
gScale :: forall c a. (ADTRecord a, Constraints a (Linear c)) => c -> a -> a

-- | Class for values supporting an inner product and various norms.
class Linear c a => Metric c a

-- | Sum of component-wise product
(<.>) :: Metric c a => a -> a -> c

-- | Maximum absolute component
norm_inf :: Metric c a => a -> c

-- | Number of non-zero components
norm_0 :: Metric c a => a -> c

-- | Sum of absolute components
norm_1 :: Metric c a => a -> c

-- | Square root of sum of squared components
norm_2 :: Metric c a => a -> c

-- | Sum of squared components
quadrance :: Metric c a => a -> c

-- | Sum of component-wise product
(<.>) :: (Metric c a, ADT a, Constraints a (Metric c)) => a -> a -> c

-- | Maximum absolute component
norm_inf :: (Metric c a, ADT a, Constraints a (Metric c), Ord c) => a -> c

-- | Number of non-zero components
norm_0 :: (Metric c a, ADT a, Constraints a (Metric c)) => a -> c

-- | Sum of absolute components
norm_1 :: (Metric c a, ADT a, Constraints a (Metric c)) => a -> c

-- | Square root of sum of squared components
norm_2 :: (Metric c a, Floating c) => a -> c

-- | Sum of squared components
quadrance :: (Metric c a, ADT a, Constraints a (Metric c)) => a -> c
infixl 7 <.>

-- | An implementation of <a>gDot</a> that works for records where every
--   field is an instance of <tt><a>Metric</a> c</tt>.
gDot :: forall c a. (ADT a, Constraints a (Metric c), Num c) => a -> a -> c

-- | An implementation of <a>norm_inf</a> that works for records where
--   every field is an instance of <tt><a>Metric</a> c</tt>.
gNorm_inf :: forall c a. (ADT a, Constraints a (Metric c), Ord c) => a -> c

-- | An implementation of <a>norm_0</a> that works for records where every
--   field is an instance of <tt><a>Metric</a> c</tt>.
gNorm_0 :: forall c a. (ADT a, Constraints a (Metric c), Num c) => a -> c

-- | An implementation of <a>norm_1</a> that works for records where every
--   field is an instance of <tt><a>Metric</a> c</tt>.
gNorm_1 :: forall c a. (ADT a, Constraints a (Metric c), Num c) => a -> c

-- | An implementation of <a>norm_2</a> that works for records where every
--   field is an instance of <tt><a>Metric</a> c</tt>.
gNorm_2 :: forall c a. (ADT a, Constraints a (Metric c), Floating c) => a -> c

-- | An implementation of <a>quadrance</a> that works for records where
--   every field is an instance of <tt><a>Metric</a> c</tt>.
gQuadrance :: forall c a. (ADT a, Constraints a (Metric c), Num c) => a -> c

-- | Instaces of <a>Linear</a> that support certain in-place mutations.
--   Inspired by the BLAS Level 1 API. A <tt><a>LinearInPlace</a> m v c
--   a</tt> means that <tt>v</tt> is a mutable reference to an <tt>a</tt>
--   that can be updated as an action in monad <tt>m</tt>.
class (Mutable m a, Linear c a) => LinearInPlace m c a

-- | Add a value in-place.
(.+.=) :: LinearInPlace m c a => Ref m a -> a -> m ()

-- | Scale a value in-place.
(.*=) :: LinearInPlace m c a => Ref m a -> c -> m ()

-- | Add a scaled value in-place.
(.*+=) :: LinearInPlace m c a => Ref m a -> (c, a) -> m ()
infix 4 .+.=
infix 4 .*=
infix 4 .*+=

-- | Given some starting reference <tt>v</tt>, add every item in a foldable
--   container into that reference in-place.
sumLinearInPlace :: (LinearInPlace m c a, Foldable t) => Ref m a -> t a -> m ()

-- | If <tt>a</tt> and <tt>b</tt> are both <a>Linear</a> instances, then if
--   <tt>a</tt> is equal to <tt>b</tt>, their scalars <tt>c</tt> and
--   <tt>d</tt> must also be equal. This is necessary because GHC isn't
--   happy with the functional dependency for some reason.
linearWit :: forall a c d. (Linear c a, Linear d a) => c :~: d
instance Data.Data.Data a => Data.Data.Data (Numeric.Opto.Update.LinearNum a)
instance GHC.Generics.Generic (Numeric.Opto.Update.LinearNum a)
instance GHC.Float.RealFloat a => GHC.Float.RealFloat (Numeric.Opto.Update.LinearNum a)
instance GHC.Real.RealFrac a => GHC.Real.RealFrac (Numeric.Opto.Update.LinearNum a)
instance GHC.Real.Integral a => GHC.Real.Integral (Numeric.Opto.Update.LinearNum a)
instance GHC.Real.Real a => GHC.Real.Real (Numeric.Opto.Update.LinearNum a)
instance GHC.Float.Floating a => GHC.Float.Floating (Numeric.Opto.Update.LinearNum a)
instance GHC.Real.Fractional a => GHC.Real.Fractional (Numeric.Opto.Update.LinearNum a)
instance GHC.Num.Num a => GHC.Num.Num (Numeric.Opto.Update.LinearNum a)
instance GHC.Enum.Bounded a => GHC.Enum.Bounded (Numeric.Opto.Update.LinearNum a)
instance GHC.Enum.Enum a => GHC.Enum.Enum (Numeric.Opto.Update.LinearNum a)
instance Data.Traversable.Traversable Numeric.Opto.Update.LinearNum
instance Data.Foldable.Foldable Numeric.Opto.Update.LinearNum
instance GHC.Base.Functor Numeric.Opto.Update.LinearNum
instance GHC.Classes.Ord a => GHC.Classes.Ord (Numeric.Opto.Update.LinearNum a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Numeric.Opto.Update.LinearNum a)
instance GHC.Show.Show a => GHC.Show.Show (Numeric.Opto.Update.LinearNum a)
instance Numeric.Opto.Update.Linear GHC.Types.Int GHC.Types.Int
instance Numeric.Opto.Update.Linear GHC.Integer.Type.Integer GHC.Integer.Type.Integer
instance Numeric.Opto.Update.Linear GHC.Real.Rational (GHC.Real.Ratio GHC.Integer.Type.Integer)
instance Numeric.Opto.Update.Linear GHC.Types.Float GHC.Types.Float
instance Numeric.Opto.Update.Linear GHC.Types.Double GHC.Types.Double
instance GHC.Float.RealFloat a => Numeric.Opto.Update.Linear (Data.Complex.Complex a) (Data.Complex.Complex a)
instance Numeric.Opto.Update.Metric GHC.Types.Int GHC.Types.Int
instance Numeric.Opto.Update.Metric GHC.Integer.Type.Integer GHC.Integer.Type.Integer
instance Numeric.Opto.Update.Metric GHC.Real.Rational (GHC.Real.Ratio GHC.Integer.Type.Integer)
instance Numeric.Opto.Update.Metric GHC.Types.Float GHC.Types.Float
instance Numeric.Opto.Update.Metric GHC.Types.Double GHC.Types.Double
instance GHC.Float.RealFloat a => Numeric.Opto.Update.Metric (Data.Complex.Complex a) (Data.Complex.Complex a)
instance Control.DeepSeq.NFData a => Control.DeepSeq.NFData (Numeric.Opto.Update.LinearNum a)
instance GHC.Num.Num a => Numeric.Opto.Update.Linear a (Numeric.Opto.Update.LinearNum a)
instance GHC.Num.Num a => Numeric.Opto.Update.Metric a (Numeric.Opto.Update.LinearNum a)
instance Numeric.Opto.Ref.Mutable m GHC.Types.Int => Numeric.Opto.Update.LinearInPlace m GHC.Types.Int GHC.Types.Int
instance Numeric.Opto.Ref.Mutable m GHC.Integer.Type.Integer => Numeric.Opto.Update.LinearInPlace m GHC.Integer.Type.Integer GHC.Integer.Type.Integer
instance Numeric.Opto.Ref.Mutable m GHC.Real.Rational => Numeric.Opto.Update.LinearInPlace m GHC.Real.Rational GHC.Real.Rational
instance Numeric.Opto.Ref.Mutable m GHC.Types.Float => Numeric.Opto.Update.LinearInPlace m GHC.Types.Float GHC.Types.Float
instance Numeric.Opto.Ref.Mutable m GHC.Types.Double => Numeric.Opto.Update.LinearInPlace m GHC.Types.Double GHC.Types.Double
instance (Numeric.Opto.Ref.Mutable m (Data.Complex.Complex a), GHC.Float.RealFloat a) => Numeric.Opto.Update.LinearInPlace m (Data.Complex.Complex a) (Data.Complex.Complex a)
instance (Control.Monad.Primitive.PrimMonad m, Control.Monad.Primitive.PrimState m Data.Type.Equality.~ s, GHC.Num.Num a, mv Data.Type.Equality.~ Data.Vector.Generic.Base.Mutable v, Data.Vector.Generic.Base.Vector v a, GHC.TypeNats.KnownNat n) => Numeric.Opto.Update.LinearInPlace m a (Data.Vector.Generic.Sized.Internal.Vector v n a)
instance (Control.Monad.Primitive.PrimMonad m, GHC.TypeNats.KnownNat n) => Numeric.Opto.Update.LinearInPlace m GHC.Types.Double (Internal.Static.R n)
instance (Control.Monad.Primitive.PrimMonad m, GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat k) => Numeric.Opto.Update.LinearInPlace m GHC.Types.Double (Internal.Static.L n k)
instance (Numeric.Opto.Ref.Mutable m (a, b), Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b) => Numeric.Opto.Update.LinearInPlace m c (a, b)
instance (Numeric.Opto.Ref.Mutable m (a, b, d), Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d) => Numeric.Opto.Update.LinearInPlace m c (a, b, d)
instance (Numeric.Opto.Ref.Mutable m (a, b, d, e), Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d, Numeric.Opto.Update.Linear c e) => Numeric.Opto.Update.LinearInPlace m c (a, b, d, e)
instance (Numeric.Opto.Ref.Mutable m (a, b, d, e, f), Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d, Numeric.Opto.Update.Linear c e, Numeric.Opto.Update.Linear c f) => Numeric.Opto.Update.LinearInPlace m c (a, b, d, e, f)
instance (GHC.Float.Floating a, GHC.Classes.Ord a, Data.Vector.Generic.Base.Vector v a, GHC.TypeNats.KnownNat n) => Numeric.Opto.Update.Metric a (Data.Vector.Generic.Sized.Internal.Vector v n a)
instance GHC.TypeNats.KnownNat n => Numeric.Opto.Update.Metric GHC.Types.Double (Internal.Static.R n)
instance (GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat m) => Numeric.Opto.Update.Metric GHC.Types.Double (Internal.Static.L n m)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (a, b)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, Numeric.Opto.Update.Metric c d, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (a, b, d)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, Numeric.Opto.Update.Metric c d, Numeric.Opto.Update.Metric c e, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (a, b, d, e)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, Numeric.Opto.Update.Metric c d, Numeric.Opto.Update.Metric c e, Numeric.Opto.Update.Metric c f, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (a, b, d, e, f)
instance (GHC.Num.Num a, Data.Vector.Generic.Base.Vector v a, GHC.TypeNats.KnownNat n) => Numeric.Opto.Update.Linear a (Data.Vector.Generic.Sized.Internal.Vector v n a)
instance GHC.TypeNats.KnownNat n => Numeric.Opto.Update.Linear GHC.Types.Double (Internal.Static.R n)
instance (GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat m) => Numeric.Opto.Update.Linear GHC.Types.Double (Internal.Static.L n m)
instance (Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b) => Numeric.Opto.Update.Linear c (a, b)
instance (Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d) => Numeric.Opto.Update.Linear c (a, b, d)
instance (Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d, Numeric.Opto.Update.Linear c e) => Numeric.Opto.Update.Linear c (a, b, d, e)
instance (Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d, Numeric.Opto.Update.Linear c e, Numeric.Opto.Update.Linear c f) => Numeric.Opto.Update.Linear c (a, b, d, e, f)


-- | Core functionality for optimizers.
module Numeric.Opto.Core

-- | Useful type synonym to indicate <i>differences</i> in <tt>a</tt> and
--   rates of change in type signatures.
type Diff a = a

-- | Gradient function to compute a direction of steepest <i>ascent</i> in
--   <tt>a</tt>, with respect to an <tt>r</tt> sample.
type Grad m r a = r -> a -> m (Diff a)

-- | An <tt><a>Opto</a> m r a</tt> represents a (potentially stateful)
--   in-place optimizer for values of type <tt>a</tt> that can be run in a
--   monad <tt>m</tt>. Each optimization step requires an additional
--   external "sample" <tt>r</tt>.
--   
--   Usually these should be defined to be polymorphic on <tt>m</tt>, so
--   that it can be run in many different contexts in
--   <a>Numeric.Opto.Run</a>.
--   
--   An <tt><a>Opto</a> m v () a</tt> is a "non-sampling" optimizer, where
--   each optimization step doesn't require any external input.
data Opto :: (Type -> Type) -> Type -> Type -> Type
[MkOpto] :: forall s m r a c. (LinearInPlace m c a, Mutable m s) => {oInit :: !s, oUpdate :: !Ref m s -> r -> a -> m (c, Diff a)} -> Opto m r a

-- | (Contravariantly) map over the type of the external sample input of an
--   <a>Opto</a>.
mapSample :: (r -> s) -> Opto m s a -> Opto m r a

-- | Map over the inner monad of an <a>Opto</a> by providing a natural
--   transformation, and also a method to "convert" the references.
mapOpto :: forall m n r a c. LinearInPlace n c a => (forall x. m x -> n x) -> (forall x. Ref n x -> Ref m x) -> Opto m r a -> Opto n r a

-- | Create an <a>Opto</a> based on a (monadic) state-updating function,
--   given an initial state and the state updating function. The function
--   takes the external <tt>r</tt> input, the current value <tt>a</tt>, the
--   current state <tt>s</tt>, and returns a step to move <tt>a</tt> in, a
--   factor to scale that step via, and an updated state.
--   
--   The state is updated in a "copying" manner (by generating new values
--   purely), without any in-place mutation.
fromCopying :: (LinearInPlace m c a, Mutable m s) => s -> (r -> a -> s -> m (c, Diff a, s)) -> Opto m r a

-- | Create a statless <a>Opto</a> based on a (monadic) optimizing
--   function. The function takes the external <tt>r</tt> input and the
--   current value <tt>a</tt> and returns a step to move <tt>a</tt> in and
--   a factor to scale that step via.
fromStateless :: LinearInPlace m c a => (r -> a -> m (c, Diff a)) -> Opto m r a

-- | Create a bona-fide <a>Grad</a> from a pure (non-monadic) sampling
--   gradient function.
pureGrad :: Applicative m => (r -> a -> Diff a) -> Grad m r a

-- | Create a <a>Grad</a> from a monadic non-sampling gradient function,
--   which ignores the external sample input <tt>r</tt>.
nonSampling :: (a -> m (Diff a)) -> Grad m r a

-- | Create a <a>Grad</a> from a pure (non-monadic) non-sampling gradient
--   function, which ignores the external sample input <tt>r</tt>.
pureNonSampling :: Applicative m => (a -> Diff a) -> Grad m r a


-- | Functions to run optimiziers.
module Numeric.Opto.Run

-- | Options for running an optimizer.
data RunOpts m a
RO :: (Diff a -> a -> m Bool) -> (a -> m ()) -> Maybe Int -> Int -> Maybe Int -> RunOpts m a

-- | Stop condition; will stop when <a>True</a> (default = never stop)
[roStopCond] :: RunOpts m a -> Diff a -> a -> m Bool

-- | Reporting function (default = no report)
[roReport] :: RunOpts m a -> a -> m ()

-- | Number of batches to run (Nothing = run forever) (default = Nothing).
[roLimit] :: RunOpts m a -> Maybe Int

-- | Size of batching updates (1 = no batching) (default = 1)
[roBatch] :: RunOpts m a -> Int

-- | batches per report (Nothing = never report) (default = Just 1).
[roFreq] :: RunOpts m a -> Maybe Int

-- | Map over the underlying monad of a <a>RunOpts</a>.
hoistRunOpts :: (forall x. m x -> n x) -> RunOpts m a -> RunOpts n a

-- | Options for running an optimizer in a concurrent setting.
data ParallelOpts m a
PO :: Maybe Int -> Int -> (NonEmpty a -> m a) -> Bool -> ParallelOpts m a

-- | Number of threads (Nothing = max capacity) (default = Nothing)
[poThreads] :: ParallelOpts m a -> Maybe Int

-- | How many batches thread will process before regrouping (default =
--   1000)
[poSplit] :: ParallelOpts m a -> Int

-- | How to recombine a pool of updated results into a single result
--   (default = <tt><a>pure</a> <a>.</a> <a>mean</a></tt>)
[poCombine] :: ParallelOpts m a -> NonEmpty a -> m a

-- | For conduit runners, whether or not conduit is in "pull-based" mode,
--   where optimization doesn't happen until requested downstream. This is
--   ignored if not running via conduit (default = True)
[poPull] :: ParallelOpts m a -> Bool

-- | Map over the underlying monad of a <a>ParallelOpts</a>.
hoistParallelOpts :: (forall x. m x -> n x) -> ParallelOpts m a -> ParallelOpts n a

-- | Run an optimizer on some input, given a monadic action to produce each
--   new sample. When the action produces <a>Nothing</a>, the running
--   immediately terminates even if the stop condition has not yet been
--   met.
opto :: Monad m => RunOpts m a -> m (Maybe r) -> a -> Opto m r a -> m a

-- | A version of <a>opto</a> that also returns an updated optimizer state
--   that can be resumed.
opto' :: Monad m => RunOpts m a -> m (Maybe r) -> a -> Opto m r a -> m (a, Opto m r a)

-- | A version of <a>optoNonSampling</a> that also returns an updated
--   optimizer state that can be resumed.
optoNonSampling' :: Monad m => RunOpts m a -> a -> Opto m () a -> m (a, Opto m () a)

-- | Run a non-sampling optimizer on some input until the stop condition is
--   met.
optoNonSampling :: Monad m => RunOpts m a -> a -> Opto m () a -> m a

-- | Given an optimizer and some initial value, produce a <a>ConduitT</a>
--   that takes in samples and outputs each successively optimized versions
--   of the value. This essentially is a convenient wrapper over
--   <a>opto</a>.
--   
--   To get the <i>final</i> optimized result after a stream has
--   terminated, compose this with a sink like <a>last</a>.
--   
--   <pre>
--   <a>optoConduit</a> ro x0 o .| <a>last</a>
--     :: ConduitT r o m (Maybe a)
--   
--   <a>optoConduit</a> ro x0 o .| <a>lastDef</a> x0
--     :: ConduitT r o m a
--   </pre>
--   
--   Note that this emits <i>every single</i> updated version of the value,
--   but still only runs <a>roReport</a> at the frequency of <a>roFreq</a>.
optoConduit :: Monad m => RunOpts m a -> a -> Opto (ConduitT r a m) r a -> ConduitT r a m ()

-- | A version of <a>optoConduit</a> that also returns an updated optimizer
--   state that can be resumed.
optoConduit' :: Monad m => RunOpts m a -> a -> Opto (ConduitT r a m) r a -> ConduitT r a m (Opto (ConduitT r a m) r a)

-- | Convenient wrapper over <a>opto</a> to allow consumption over a list
--   of samples.
optoFold :: Monad m => RunOpts m a -> a -> Opto (StateT [r] m) r a -> [r] -> m (a, [r])

-- | A version of <a>optoFold'</a> that also returns an updated optimizer
--   state that can be resumed.
optoFold' :: Monad m => RunOpts m a -> a -> Opto (StateT [r] m) r a -> [r] -> m (a, [r], Opto (StateT [r] m) r a)

-- | Run an optimizer in parallel on multiple threads on some value, given
--   a (thread-safe) monadic action to produce each new sample.
--   
--   It does this by repeatedly:
--   
--   <ol>
--   <li>Splitting into multiple threads (based on <a>poThreads</a>)</li>
--   <li>Running <a>opto</a> (single-threaded optimiztion) on each thread,
--   independently, from the same initial value.</li>
--   <li>After <a>poSplit</a> items have been processed, all threads wait
--   on each other to stop. After each thread is done, each thread's
--   optimized value is then aggregated using <a>poCombine</a> (by default,
--   it takes the mean).</li>
--   <li>This new optimized combined value is then used to begin the cycle
--   again.</li>
--   </ol>
--   
--   When action produces <a>Nothing</a> for <i>all</i> threads, the
--   running immediately terminates on all threads and returns even if the
--   stop condition has not yet been met. If the stop condition is met, the
--   value given to the stop condition will be used as the final result,
--   ignoring all other thread pools.
optoPar :: forall m r a. MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> m (Maybe r) -> a -> Opto m r a -> m a

-- | A version of <a>optoPar</a> that performs a batch fetch for each
--   thread's entire sample pool <i>before</i> beginning parallel
--   optimization. This can be useful if the sampling is faster in batch
--   amounts.
optoParChunk :: forall m r a. MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> (Int -> m [r]) -> a -> Opto (StateT [r] m) r a -> m a

-- | Run a non-sampling optimizer in parallel on multiple threads on some
--   value until the stop condition is met.
--   
--   See <a>optoPar</a> for a detailed description of how parallel
--   optimization is implemented.
optoParNonSampling :: MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> a -> Opto m () a -> m a

-- | Given an optimizer, some initial value, and a conduit <i>source</i>,
--   returns a conduit sorce that outputs succesively optimized versions of
--   the value after each thread recombination, where each version is
--   optimized using parallel multi-threaded optimization.
--   
--   See <a>optoPar</a> for a detailed description on how parallel
--   optimization is implemented.
--   
--   Note that, unlike <a>optoConduit</a>, which is a conduit, this is a
--   conduit (source) <i>transformer</i>. It takes a source outputting
--   <i>samples</i> and returns a <i>new</i> source of <i>optimized
--   values</i>.
--   
--   A value is emitted after every thread recombination/call of
--   <a>poCombine</a>.
optoConduitPar :: forall m r a. MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> a -> Opto m r a -> ConduitT () r m () -> ConduitT () a m ()

-- | A version of <a>optoConduitPar</a> that performs a batch fetch from
--   the input source for each thread's entire sample pool <i>before</i>
--   beginning parallel optimization. This can be useful if the source can
--   produce values faster in batch amounts.
optoConduitParChunk :: forall m r a. MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> a -> Opto (StateT [r] m) r a -> ConduitT () r m () -> ConduitT () a m ()

-- | The mean of the values in a non-empty container.
mean :: (Foldable1 t, Fractional a) => t a -> a
instance (GHC.Num.Num a, GHC.Num.Num b) => GHC.Base.Semigroup (Numeric.Opto.Run.Sum2 a b)
instance (GHC.Num.Num a, GHC.Num.Num b) => GHC.Base.Monoid (Numeric.Opto.Run.Sum2 a b)
instance (GHC.Base.Applicative m, GHC.Real.Fractional a) => Data.Default.Class.Default (Numeric.Opto.Run.ParallelOpts m a)
instance GHC.Base.Functor m => Data.Functor.Invariant.Invariant (Numeric.Opto.Run.ParallelOpts m)
instance GHC.Base.Applicative m => Data.Default.Class.Default (Numeric.Opto.Run.RunOpts m a)
instance Data.Functor.Contravariant.Contravariant (Numeric.Opto.Run.RunOpts m)
instance Data.Functor.Invariant.Invariant (Numeric.Opto.Run.RunOpts m)

module Numeric.Opto.Run.Simple

-- | A sample conduit <a>Source</a> that yields shuffled items in a
--   collection repeatedly. Deposits the yielded items in a <a>TBQueue</a>,
--   and takes a callback for reporting each new epoch (cycle of items in
--   the collection).
sampleCollect :: (PrimMonad m, MonadIO m, MonoFoldable t) => TBQueue (Element t) -> Maybe Int -> (Int -> m ()) -> t -> Gen (PrimState m) -> ConduitT i (Element t) m ()

-- | A conduit that processes "trained" items and outputs a report every
--   report batch, based on samples accumulated in a <a>TBQueue</a>. Meant
--   to be used alongside <a>sampleCollect</a>. It passes through all of
--   its input.
--   
--   <pre>
--      <a>sampleCollect</a> sampleQueue epochs <a>dispEpoch</a> samples
--   .| <a>optoConduit</a> ro model0 optimizer
--   .| <a>trainReport</a> sampleQueue <a>dispBatch</a> 2500 (<a>simpleReport</a> (<a>Just</a> valSet) runTest)
--   .| sink -- (other stuff you would want to do with trained models)
--   </pre>
trainReport :: (MonadIO m, NFData a) => TBQueue i -> (Int -> m ()) -> Int -> (NominalDiffTime -> [i] -> a -> m ()) -> ConduitT a a m ()

-- | Simple reporter to give to <a>sampleCollect</a>:
--   
--   <pre>
--   [Epoch 1]
--   [Epoch 2]
--   [Epoch 3]
--   </pre>
dispEpoch :: MonadIO m => Int -> m ()

-- | Simple reporter to give to <a>trainReport</a>:
--   
--   <pre>
--   (Batch 1)
--   (Batch 2)
--   (Batch 3)
--   </pre>
dispBatch :: MonadIO m => Int -> m ()

-- | Simple reporting callback to provide <a>trainReport</a>.
simpleReport :: MonadIO m => Maybe [i] -> ([i] -> a -> String) -> NominalDiffTime -> [i] -> a -> m ()

-- | Integrate <a>sampleCollect</a> and <a>trainReport</a> together,
--   automatically generating the sample queue and supplying all callbacks
--   based on the <a>SimpleOpts</a>.
simpleRunner :: forall m t a b n. (MonadIO m, PrimMonad m, NFData a, MonoFoldable t) => SimpleOpts m (Element t) a b -> t -> SOOptimizer m n (Element t) a -> RunOpts m a -> a -> Opto n (Element t) a -> Gen (PrimState m) -> m b

-- | Options for <a>simpleRunner</a>. <a>def</a> gives sensible defaults.
data SimpleOpts m i a b
SO :: Maybe Int -> (Int -> m ()) -> (Int -> m ()) -> Maybe [i] -> Int -> ([i] -> a -> String) -> ConduitT a Void m b -> SimpleOpts m i a b

-- | How many epochs (default: Nothing, forever)
[soEpochs] :: SimpleOpts m i a b -> Maybe Int

-- | Display a new epoch to the screen (default: "[Epoch %d]")
[soDispEpoch] :: SimpleOpts m i a b -> Int -> m ()

-- | Display a new report batch to the screen (defualt: "(Batch %d)")
[soDispBatch] :: SimpleOpts m i a b -> Int -> m ()

-- | Test set to validate results (default: Nothing)
[soTestSet] :: SimpleOpts m i a b -> Maybe [i]

-- | Number of samples to skip per report batch (default: 1000)
[soSkipSamps] :: SimpleOpts m i a b -> Int

-- | Evaluate a model against samples and report accuracy. (default: do
--   nothing)
[soEvaluate] :: SimpleOpts m i a b -> [i] -> a -> String

-- | Collect the optimized values (default: <a>sinkNull</a>, ignore them)
[soSink] :: SimpleOpts m i a b -> ConduitT a Void m b

-- | Choose a concurrency strategy for your runner.
data SOOptimizer :: (Type -> Type) -> (Type -> Type) -> Type -> Type -> Type

-- | Single-threaded
[SOSingle] :: SOOptimizer m (ConduitT i a m) i a

-- | Parallel
[SOParallel] :: MonadUnliftIO m => ParallelOpts m a -> SOOptimizer m m i a

-- | Parallel chunked
[SOParChunked] :: MonadUnliftIO m => ParallelOpts m a -> SOOptimizer m (StateT [i] m) i a
instance (Control.Monad.IO.Class.MonadIO m, Data.Default.Class.Default b) => Data.Default.Class.Default (Numeric.Opto.Run.Simple.SimpleOpts m i a b)


-- | Defining various numeric optimizers. Most of these implemtations are
--   taken directly from
--   <a>http://ruder.io/optimizing-gradient-descent/</a>
module Numeric.Opto.Optimizer

-- | Steepest descent, acording to some learning rate. The simplest
--   optimizer.
steepestDescent :: LinearInPlace m c a => c -> Grad m r a -> Opto m r a

-- | Hyperparameter for <a>momentum</a>
newtype Momentum c
Momentum :: c -> Momentum c
[momentumDecay] :: Momentum c -> c

-- | Steepest descent with momentum. (Qian, 1999)
momentum :: forall m r a c. LinearInPlace m c a => Momentum c -> c -> Grad m r a -> Opto m r a

-- | Hyperparameter for <a>nesterov</a>
newtype Nesterov c
Nesterov :: c -> Nesterov c
[nesterovDecay] :: Nesterov c -> c

-- | Nesterov accelerated gradient (NAG) (Nesterov, 1983)
nesterov :: forall m r a c. LinearInPlace m c a => Nesterov c -> c -> Grad m r a -> Opto m r a

-- | Hyperparameters for <a>adagrad</a>
data Adagrad c
Adagrad :: c -> c -> Adagrad c
[adagradRate] :: Adagrad c -> c
[adagradEps] :: Adagrad c -> c

-- | Adaptive Gradient (Duchu, Hazan, Singer, 2011). Note that if the state
--   is not reset periodically, updates tend to zero fairly quickly.
adagrad :: forall m r a c. (LinearInPlace m c a, Floating a, Real c) => Adagrad c -> Grad m r a -> Opto m r a

-- | Hyperparameters for <a>adadelta</a>
data Adadelta c
Adadelta :: c -> c -> Adadelta c
[adadeltaDecay] :: Adadelta c -> c
[adadeltaEps] :: Adadelta c -> c

-- | The Adadelta extension of Adagrad (Zeiler, 2012) that mitigates the
--   decreasing learning rate.
adadelta :: forall m r a c. (LinearInPlace m c a, Floating a, Real c) => Adadelta c -> Grad m r a -> Opto m r a

-- | Hyperparameters for <a>rmsProp</a>
data RMSProp c
RMSProp :: c -> c -> c -> RMSProp c
[rmsPropRate] :: RMSProp c -> c
[rmsPropDecay] :: RMSProp c -> c
[rmsPropEps] :: RMSProp c -> c

-- | RMSProp, as described by Geoff Hinton.
rmsProp :: forall m r a c. (LinearInPlace m c a, Floating a, Real c) => RMSProp c -> Grad m r a -> Opto m r a

-- | Hyperparameters for <a>adam</a>
data Adam c
Adam :: !c -> !c -> !c -> !c -> Adam c
[adamStep] :: Adam c -> !c
[adamDecay1] :: Adam c -> !c
[adamDecay2] :: Adam c -> !c
[adamEps] :: Adam c -> !c

-- | Adaptive Moment Estimation (Kingma, Ba, 2015)
adam :: forall m r a c. (RealFloat c, Floating a, LinearInPlace m c a, Mutable m c) => Adam c -> Grad m r a -> Opto m r a

-- | Hyperparameters for <a>adaMax</a>
data AdaMax c
AdaMax :: !c -> !c -> !c -> !c -> AdaMax c
[adaMaxStep] :: AdaMax c -> !c
[adaMaxDecay1] :: AdaMax c -> !c
[adaMaxDecay2] :: AdaMax c -> !c
[adaMaxEps] :: AdaMax c -> !c

-- | Adam variation (Kingma and Ba, 2015)
adaMax :: forall m r a c. (RealFloat c, Metric c a, LinearInPlace m c a, Mutable m c) => AdaMax c -> Grad m r a -> Opto m r a
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.AdaMax c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.AdaMax c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.Adam c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.Adam c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.RMSProp c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.RMSProp c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.Adadelta c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.Adadelta c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.Adagrad c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.Adagrad c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.Nesterov c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.Nesterov c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.Momentum c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.Momentum c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.AdaMax c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.Adam c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.RMSProp c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.Adadelta c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.Adagrad c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.Nesterov c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.Momentum c)


-- | Generate gradients usable with <a>Numeric.Opto</a> using the
--   <i>backprop</i> library.
module Numeric.Opto.Backprop

-- | Turn a simple <tt>a -&gt; b</tt> function into a <tt><a>Grad</a> m
--   a</tt>.
bpGrad :: (Monad m, Backprop a, Backprop b) => (forall s. Reifies s W => BVar s a -> BVar s b) -> Grad m r a

-- | Turn a <tt>a -&gt; b</tt> function parameterized on <tt>r</tt> into a
--   <tt><a>Grad</a> m a</tt>.
bpGradSample :: (Backprop a, Backprop b, Applicative m) => (forall s. Reifies s W => r -> BVar s a -> BVar s b) -> Grad m r a


-- | Main library entrypoint; essentially re-exports all of the submodules
--   to provide full functionality with a single import.
module Numeric.Opto
