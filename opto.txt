-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | General-purpose performant numeric optimization library
--   
--   Please see the README on Github at
--   <a>https://github.com/mstksg/opto#readme</a>
@package opto
@version 0.1.0.0


-- | Abstract over different types for mutable references of values.
module Numeric.Opto.Ref

-- | Abstraction over types of mutable references for values. A
--   <tt><a>Ref</a> m a v</tt> means that a <tt>v</tt> is a mutable
--   reference to an <tt>a</tt>, and we may update<i>write</i>modify
--   <tt>v</tt> in context <tt>m</tt>.
--   
--   This allows us to reat mutable vectors and in-place mutable numbers or
--   records in the same way.
class Monad m => Ref m a v | v -> a

-- | Initialize a mutable reference with a given value
thawRef :: Ref m a v => a -> m v

-- | Read an immutable value back from a mutable reference
freezeRef :: Ref m a v => v -> m a

-- | Copy an immutable value into a mutable reference
copyRef :: Ref m a v => v -> a -> m ()

-- | Apply a pure function on an immutable value onto a value stored in a
--   mutable reference.
modifyRef :: Ref m a v => v -> (a -> a) -> m ()

-- | <a>modifyRef</a>, but forces the result before storing it back in the
--   reference.
modifyRef' :: Ref m a v => v -> (a -> a) -> m ()

-- | Apply a pure function on an immutable value onto a value stored in a
--   mutable reference, returning a result value from that function.
updateRef :: Ref m a v => v -> (a -> (a, b)) -> m b

-- | <a>updateRef</a>, but forces the updated value before storing it back
--   in the reference.
updateRef' :: Ref m a v => v -> (a -> (a, b)) -> m b
instance (Control.Monad.Primitive.PrimMonad m, Control.Monad.Primitive.PrimState m Data.Type.Equality.~ s) => Numeric.Opto.Ref.Ref m a (Data.Primitive.MutVar.MutVar s a)
instance (Control.Monad.Primitive.PrimMonad m, Control.Monad.Primitive.PrimState m Data.Type.Equality.~ s) => Numeric.Opto.Ref.Ref m (Data.Vector.Vector a) (Data.Vector.Mutable.MVector s a)
instance (Control.Monad.Primitive.PrimMonad m, Data.Vector.Generic.Base.Mutable v Data.Type.Equality.~ mv, Control.Monad.Primitive.PrimState m Data.Type.Equality.~ s, Data.Vector.Generic.Base.Vector v a) => Numeric.Opto.Ref.Ref m (Data.Vector.Generic.Sized.Internal.Vector v n a) (Data.Vector.Generic.Mutable.Sized.Internal.MVector mv n s a)
instance GHC.Base.Monad m => Numeric.Opto.Ref.Ref m () ()
instance (GHC.Base.Monad m, Numeric.Opto.Ref.Ref m a u, Numeric.Opto.Ref.Ref m b v) => Numeric.Opto.Ref.Ref m (a, b) (u, v)
instance (GHC.Base.Monad m, Numeric.Opto.Ref.Ref m a u, Numeric.Opto.Ref.Ref m b v, Numeric.Opto.Ref.Ref m c w) => Numeric.Opto.Ref.Ref m (a, b, c) (u, v, w)
instance (GHC.Base.Monad m, Numeric.Opto.Ref.Ref m a u, Numeric.Opto.Ref.Ref m b v, Numeric.Opto.Ref.Ref m c w, Numeric.Opto.Ref.Ref m d j) => Numeric.Opto.Ref.Ref m (a, b, c, d) (u, v, w, j)


-- | Conduits that are useful for sampling and running optimizers.
module Numeric.Opto.Run.Conduit

-- | Outputs a shuffled version of the input stream. Keeps entire input
--   stream in memory.
--   
--   NOTE: Pulls the entire input stream into memory first before
--   outputting anything.
shuffling :: PrimMonad m => Gen (PrimState m) -> ConduitT a a m ()

-- | Takes the first N items out of the input stream, shuffles them
--   in-memory, and outputs the shuffled result.
--   
--   Leaves the rest of the items in the stream.
--   
--   Use <a>forever</a> to repeat multiple times until the stream is
--   exhausted.
shufflingN :: PrimMonad m => Int -> Gen (PrimState m) -> ConduitT a a m ()

-- | Process an entire stream, and keep N random and shuffled items from
--   that stream. Is O(N) memory.
sinkSampleReservoir :: forall m v a o. (PrimMonad m, Vector v a) => Int -> Gen (PrimState m) -> ConduitT a o m (v a)

-- | Process an entire stream, and yield N random items from that stream.
--   Is O(N) memory.
--   
--   NOTE: Exhausts the entire input stream first before outputting
--   anything, but never keeps the entire original stream in memory.
samplingN :: PrimMonad m => Int -> Gen (PrimState m) -> ConduitT a a m ()

-- | Drops and lets items through randomly with a given probability.
skipSampling :: PrimMonad m => Double -> Gen (PrimState m) -> ConduitT a a m ()


-- | A unified interface for values in vector spaces that can be added and
--   scaled (purely and also in-place), and measured.
module Numeric.Opto.Update

-- | If <tt>a</tt> is an instance of <tt><a>Linear</a> c</tt>, you can
--   <i>add</i> together values of <tt>a</tt>, and <i>scale</i> them using
--   <tt>c</tt>s.
--   
--   For example, if you have a vector of doubles, you can add them
--   together component-wise, and scale them by multiplying every item by
--   the scalar.
--   
--   Mathematically, this means that <tt>a</tt> forms something like a
--   module or vector space over <tt>c</tt>, where <tt>c</tt> can be any
--   <a>Num</a> instance.
class Num c => Linear c a | a -> c

-- | Add together <tt>a</tt>s. Should be associative.
--   
--   <pre>
--   x .+. (y .+. z) == (x .+. y) .+. z
--   </pre>
--   
--   If <tt>a</tt> is an instance of <a>Num</a>, this can be just <a>+</a>.
(.+.) :: Linear c a => a -> a -> a

-- | The "zero" <tt>a</tt>, meant to form an identity with <a>.+.</a>.
--   
--   <pre>
--   x .+. zeroL == x
--   zeroL .+. y == y
--   </pre>
--   
--   If <tt>a</tt> is an instance of <a>Num</a>, this can be just 0.
zeroL :: Linear c a => a

-- | Scale an <tt>a</tt> by a factor <tt>c</tt>. Should distribute over
--   <a>.+.</a>.
--   
--   <pre>
--   a .* (x .+. y) == (a .* x) .+. (a .* y)
--   a .* (b .* c)  == (a * b) .* c
--   </pre>
(.*) :: Linear c a => c -> a -> a

-- | Add together <tt>a</tt>s. Should be associative.
--   
--   <pre>
--   x .+. (y .+. z) == (x .+. y) .+. z
--   </pre>
--   
--   If <tt>a</tt> is an instance of <a>Num</a>, this can be just <a>+</a>.
(.+.) :: (Linear c a, ADTRecord a, Constraints a (Linear c)) => a -> a -> a

-- | The "zero" <tt>a</tt>, meant to form an identity with <a>.+.</a>.
--   
--   <pre>
--   x .+. zeroL == x
--   zeroL .+. y == y
--   </pre>
--   
--   If <tt>a</tt> is an instance of <a>Num</a>, this can be just 0.
zeroL :: (Linear c a, ADTRecord a, Constraints a (Linear c)) => a

-- | Scale an <tt>a</tt> by a factor <tt>c</tt>. Should distribute over
--   <a>.+.</a>.
--   
--   <pre>
--   a .* (x .+. y) == (a .* x) .+. (a .* y)
--   a .* (b .* c)  == (a * b) .* c
--   </pre>
(.*) :: (Linear c a, ADTRecord a, Constraints a (Linear c)) => c -> a -> a
infixl 6 .+.
infixl 7 .*

-- | Sum over a <a>Foldable</a> container of <tt><a>Linear</a> c a</tt>
sumLinear :: (Linear c a, Foldable t) => t a -> a

-- | An implementation of <a>.+.</a> that works for records where every
--   field is an instance of <tt><a>Linear</a> c</tt> (that is, every field
--   is additive and can be scaled by the same <tt>c</tt>).
gAdd :: forall c a. (ADTRecord a, Constraints a (Linear c)) => a -> a -> a

-- | An implementation of <a>zeroL</a> that works for records where every
--   field is an instance of <tt><a>Linear</a> c</tt> (that is, every field
--   is additive and can be scaled by the same <tt>c</tt>).
gZeroL :: forall c a. (ADTRecord a, Constraints a (Linear c)) => a

-- | An implementation of <a>.*</a> that works for records where every
--   field is an instance of <tt><a>Linear</a> c</tt> (that is, every field
--   is additive and can be scaled by the same <tt>c</tt>).
gScale :: forall c a. (ADTRecord a, Constraints a (Linear c)) => c -> a -> a

-- | Class for values supporting an inner product and various norms.
class Linear c a => Metric c a

-- | Sum of component-wise product
(<.>) :: Metric c a => a -> a -> c

-- | Maximum absolute component
norm_inf :: Metric c a => a -> c

-- | Number of non-zero components
norm_0 :: Metric c a => a -> c

-- | Sum of absolute components
norm_1 :: Metric c a => a -> c

-- | Square root of sum of squared components
norm_2 :: Metric c a => a -> c

-- | Sum of squared components
quadrance :: Metric c a => a -> c

-- | Sum of component-wise product
(<.>) :: (Metric c a, ADT a, Constraints a (Metric c)) => a -> a -> c

-- | Maximum absolute component
norm_inf :: (Metric c a, ADT a, Constraints a (Metric c), Ord c) => a -> c

-- | Number of non-zero components
norm_0 :: (Metric c a, ADT a, Constraints a (Metric c)) => a -> c

-- | Sum of absolute components
norm_1 :: (Metric c a, ADT a, Constraints a (Metric c)) => a -> c

-- | Square root of sum of squared components
norm_2 :: (Metric c a, Floating c) => a -> c

-- | Sum of squared components
quadrance :: (Metric c a, ADT a, Constraints a (Metric c)) => a -> c
infixl 7 <.>

-- | An implementation of <a>gDot</a> that works for records where every
--   field is an instance of <tt><a>Metric</a> c</tt>.
gDot :: forall c a. (ADT a, Constraints a (Metric c), Num c) => a -> a -> c

-- | An implementation of <a>norm_inf</a> that works for records where
--   every field is an instance of <tt><a>Metric</a> c</tt>.
gNorm_inf :: forall c a. (ADT a, Constraints a (Metric c), Ord c) => a -> c

-- | An implementation of <a>norm_0</a> that works for records where every
--   field is an instance of <tt><a>Metric</a> c</tt>.
gNorm_0 :: forall c a. (ADT a, Constraints a (Metric c), Num c) => a -> c

-- | An implementation of <a>norm_1</a> that works for records where every
--   field is an instance of <tt><a>Metric</a> c</tt>.
gNorm_1 :: forall c a. (ADT a, Constraints a (Metric c), Num c) => a -> c

-- | An implementation of <a>norm_2</a> that works for records where every
--   field is an instance of <tt><a>Metric</a> c</tt>.
gNorm_2 :: forall c a. (ADT a, Constraints a (Metric c), Floating c) => a -> c

-- | An implementation of <a>quadrance</a> that works for records where
--   every field is an instance of <tt><a>Metric</a> c</tt>.
gQuadrance :: forall c a. (ADT a, Constraints a (Metric c), Num c) => a -> c

-- | Instaces of <a>Linear</a> that support certain in-place mutations.
--   Inspired by the BLAS Level 1 API. A <tt><a>LinearInPlace</a> m v c
--   a</tt> means that <tt>v</tt> is a mutable reference to an <tt>a</tt>
--   that can be updated as an action in monad <tt>m</tt>.
class (Ref m a v, Linear c a) => LinearInPlace m v c a

-- | Add a value in-place.
(.+.=) :: LinearInPlace m v c a => v -> a -> m ()

-- | Scale a value in-place.
(.*=) :: LinearInPlace m v c a => v -> c -> m ()

-- | Add a scaled value in-place.
(.*+=) :: LinearInPlace m v c a => v -> (c, a) -> m ()
infix 4 .+.=
infix 4 .*=
infix 4 .*+=

-- | Given some starting reference <tt>v</tt>, add every item in a foldable
--   container into that reference in-place.
sumLinearInPlace :: (LinearInPlace m v c a, Foldable t) => v -> t a -> m ()
instance Data.Data.Data a => Data.Data.Data (Numeric.Opto.Update.LinearNum a)
instance GHC.Generics.Generic (Numeric.Opto.Update.LinearNum a)
instance GHC.Float.RealFloat a => GHC.Float.RealFloat (Numeric.Opto.Update.LinearNum a)
instance GHC.Real.RealFrac a => GHC.Real.RealFrac (Numeric.Opto.Update.LinearNum a)
instance GHC.Real.Integral a => GHC.Real.Integral (Numeric.Opto.Update.LinearNum a)
instance GHC.Real.Real a => GHC.Real.Real (Numeric.Opto.Update.LinearNum a)
instance GHC.Float.Floating a => GHC.Float.Floating (Numeric.Opto.Update.LinearNum a)
instance GHC.Real.Fractional a => GHC.Real.Fractional (Numeric.Opto.Update.LinearNum a)
instance GHC.Num.Num a => GHC.Num.Num (Numeric.Opto.Update.LinearNum a)
instance GHC.Enum.Bounded a => GHC.Enum.Bounded (Numeric.Opto.Update.LinearNum a)
instance GHC.Enum.Enum a => GHC.Enum.Enum (Numeric.Opto.Update.LinearNum a)
instance Data.Traversable.Traversable Numeric.Opto.Update.LinearNum
instance Data.Foldable.Foldable Numeric.Opto.Update.LinearNum
instance GHC.Base.Functor Numeric.Opto.Update.LinearNum
instance GHC.Classes.Ord a => GHC.Classes.Ord (Numeric.Opto.Update.LinearNum a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Numeric.Opto.Update.LinearNum a)
instance GHC.Show.Show a => GHC.Show.Show (Numeric.Opto.Update.LinearNum a)
instance Numeric.Opto.Update.Linear GHC.Types.Int GHC.Types.Int
instance Numeric.Opto.Update.Linear GHC.Integer.Type.Integer GHC.Integer.Type.Integer
instance Numeric.Opto.Update.Linear GHC.Real.Rational (GHC.Real.Ratio GHC.Integer.Type.Integer)
instance Numeric.Opto.Update.Linear GHC.Types.Float GHC.Types.Float
instance Numeric.Opto.Update.Linear GHC.Types.Double GHC.Types.Double
instance GHC.Float.RealFloat a => Numeric.Opto.Update.Linear (Data.Complex.Complex a) (Data.Complex.Complex a)
instance Numeric.Opto.Update.Metric GHC.Types.Int GHC.Types.Int
instance Numeric.Opto.Update.Metric GHC.Integer.Type.Integer GHC.Integer.Type.Integer
instance Numeric.Opto.Update.Metric GHC.Real.Rational (GHC.Real.Ratio GHC.Integer.Type.Integer)
instance Numeric.Opto.Update.Metric GHC.Types.Float GHC.Types.Float
instance Numeric.Opto.Update.Metric GHC.Types.Double GHC.Types.Double
instance GHC.Float.RealFloat a => Numeric.Opto.Update.Metric (Data.Complex.Complex a) (Data.Complex.Complex a)
instance Control.DeepSeq.NFData a => Control.DeepSeq.NFData (Numeric.Opto.Update.LinearNum a)
instance GHC.Num.Num a => Numeric.Opto.Update.Linear a (Numeric.Opto.Update.LinearNum a)
instance GHC.Num.Num a => Numeric.Opto.Update.Metric a (Numeric.Opto.Update.LinearNum a)
instance Numeric.Opto.Ref.Ref m GHC.Types.Int v => Numeric.Opto.Update.LinearInPlace m v GHC.Types.Int GHC.Types.Int
instance Numeric.Opto.Ref.Ref m GHC.Integer.Type.Integer v => Numeric.Opto.Update.LinearInPlace m v GHC.Integer.Type.Integer GHC.Integer.Type.Integer
instance Numeric.Opto.Ref.Ref m GHC.Real.Rational v => Numeric.Opto.Update.LinearInPlace m v GHC.Real.Rational GHC.Real.Rational
instance Numeric.Opto.Ref.Ref m GHC.Types.Float v => Numeric.Opto.Update.LinearInPlace m v GHC.Types.Float GHC.Types.Float
instance Numeric.Opto.Ref.Ref m GHC.Types.Double v => Numeric.Opto.Update.LinearInPlace m v GHC.Types.Double GHC.Types.Double
instance (Numeric.Opto.Ref.Ref m (Data.Complex.Complex a) v, GHC.Float.RealFloat a) => Numeric.Opto.Update.LinearInPlace m v (Data.Complex.Complex a) (Data.Complex.Complex a)
instance (Control.Monad.Primitive.PrimMonad m, Control.Monad.Primitive.PrimState m Data.Type.Equality.~ s, GHC.Num.Num a, mv Data.Type.Equality.~ Data.Vector.Generic.Base.Mutable v, Data.Vector.Generic.Base.Vector v a, GHC.TypeNats.KnownNat n) => Numeric.Opto.Update.LinearInPlace m (Data.Vector.Generic.Mutable.Sized.Internal.MVector mv n s a) a (Data.Vector.Generic.Sized.Internal.Vector v n a)
instance (GHC.TypeNats.KnownNat n, Numeric.Opto.Ref.Ref m (Internal.Static.R n) v) => Numeric.Opto.Update.LinearInPlace m v GHC.Types.Double (Internal.Static.R n)
instance (GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat k, Numeric.Opto.Ref.Ref m (Internal.Static.L n k) v) => Numeric.Opto.Update.LinearInPlace m v GHC.Types.Double (Internal.Static.L n k)
instance (Numeric.Opto.Ref.Ref m (a, b) v, Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b) => Numeric.Opto.Update.LinearInPlace m v c (a, b)
instance (Numeric.Opto.Ref.Ref m (a, b, d) v, Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d) => Numeric.Opto.Update.LinearInPlace m v c (a, b, d)
instance (Numeric.Opto.Ref.Ref m (a, b, d, e) v, Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d, Numeric.Opto.Update.Linear c e) => Numeric.Opto.Update.LinearInPlace m v c (a, b, d, e)
instance (Numeric.Opto.Ref.Ref m (a, b, d, e, f) v, Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d, Numeric.Opto.Update.Linear c e, Numeric.Opto.Update.Linear c f) => Numeric.Opto.Update.LinearInPlace m v c (a, b, d, e, f)
instance (GHC.Float.Floating a, GHC.Classes.Ord a, Data.Vector.Generic.Base.Vector v a, GHC.TypeNats.KnownNat n) => Numeric.Opto.Update.Metric a (Data.Vector.Generic.Sized.Internal.Vector v n a)
instance GHC.TypeNats.KnownNat n => Numeric.Opto.Update.Metric GHC.Types.Double (Internal.Static.R n)
instance (GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat m) => Numeric.Opto.Update.Metric GHC.Types.Double (Internal.Static.L n m)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (a, b)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, Numeric.Opto.Update.Metric c d, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (a, b, d)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, Numeric.Opto.Update.Metric c d, Numeric.Opto.Update.Metric c e, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (a, b, d, e)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, Numeric.Opto.Update.Metric c d, Numeric.Opto.Update.Metric c e, Numeric.Opto.Update.Metric c f, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (a, b, d, e, f)
instance (GHC.Num.Num a, Data.Vector.Generic.Base.Vector v a, GHC.TypeNats.KnownNat n) => Numeric.Opto.Update.Linear a (Data.Vector.Generic.Sized.Internal.Vector v n a)
instance GHC.TypeNats.KnownNat n => Numeric.Opto.Update.Linear GHC.Types.Double (Internal.Static.R n)
instance (GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat m) => Numeric.Opto.Update.Linear GHC.Types.Double (Internal.Static.L n m)
instance (Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b) => Numeric.Opto.Update.Linear c (a, b)
instance (Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d) => Numeric.Opto.Update.Linear c (a, b, d)
instance (Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d, Numeric.Opto.Update.Linear c e) => Numeric.Opto.Update.Linear c (a, b, d, e)
instance (Numeric.Opto.Update.Linear c a, Numeric.Opto.Update.Linear c b, Numeric.Opto.Update.Linear c d, Numeric.Opto.Update.Linear c e, Numeric.Opto.Update.Linear c f) => Numeric.Opto.Update.Linear c (a, b, d, e, f)


-- | Core functionality for optimizers.
module Numeric.Opto.Core

-- | Useful type synonym to indicate <i>differences</i> in <tt>a</tt> and
--   rates of change in type signatures.
type Diff a = a

-- | Gradient function to compute a direction of steepest <i>ascent</i> in
--   <tt>a</tt>, with respect to an <tt>r</tt> sample.
type Grad m r a = r -> a -> m (Diff a)

-- | An <tt><a>Opto</a> m v r a</tt> represents a (potentially stateful)
--   in-place optimizer for values of type <tt>a</tt>, stored in mutable
--   references <tt>v</tt>, that can be run in a monad <tt>m</tt>. Each
--   optimization step requires an additional external "sample" <tt>r</tt>.
--   
--   Usually these should be defined to be polymorphic on <tt>m</tt>, so
--   that it can be run in many different contexts in
--   <a>Numeric.Opto.Run</a>.
--   
--   An <tt><a>Opto</a> m v () a</tt> is a "non-sampling" optimizer, where
--   each optimization step doesn't require any external input.
data Opto :: (Type -> Type) -> Type -> Type -> Type -> Type
[MkOpto] :: forall s u m v r a c. (LinearInPlace m v c a, Ref m s u) => {oInit :: !s, oUpdate :: !u -> r -> a -> m (c, Diff a)} -> Opto m v r a

-- | (Contravariantly) map over the type of the external sample input of an
--   <a>Opto</a>.
mapSample :: (r -> s) -> Opto m v s a -> Opto m v r a

-- | Create an <a>Opto</a> based on a (monadic) state-updating function,
--   given an initial state and the state updating function. The function
--   takes the external <tt>r</tt> input, the current value <tt>a</tt>, the
--   current state <tt>s</tt>, and returns a step to move <tt>a</tt> in, a
--   factor to scale that step via, and an updated state.
--   
--   The state is updated in a "copying" manner (by generating new values
--   purely), without any in-place mutation.
fromCopying :: (PrimMonad m, LinearInPlace m v c a) => s -> (r -> a -> s -> m (c, Diff a, s)) -> Opto m v r a

-- | Create a statless <a>Opto</a> based on a (monadic) optimizing
--   function. The function takes the external <tt>r</tt> input and the
--   current value <tt>a</tt> and returns a step to move <tt>a</tt> in and
--   a factor to scale that step via.
fromStateless :: LinearInPlace m v c a => (r -> a -> m (c, Diff a)) -> Opto m v r a

-- | Create a bona-fide <a>Grad</a> from a pure (non-monadic) sampling
--   gradient function.
pureGrad :: Applicative m => (r -> a -> Diff a) -> Grad m r a

-- | Create a <a>Grad</a> from a monadic non-sampling gradient function,
--   which ignores the external sample input <tt>r</tt>.
nonSampling :: (a -> m (Diff a)) -> Grad m r a

-- | Create a <a>Grad</a> from a pure (non-monadic) non-sampling gradient
--   function, which ignores the external sample input <tt>r</tt>.
pureNonSampling :: Applicative m => (a -> Diff a) -> Grad m r a


-- | Functions to run optimiziers.
module Numeric.Opto.Run

-- | Options for running an optimizer.
data RunOpts m a
RO :: (Diff a -> a -> m Bool) -> (a -> m ()) -> Maybe Int -> Int -> Maybe Int -> RunOpts m a

-- | Stop condition; will stop when <a>True</a> (default = never stop)
[roStopCond] :: RunOpts m a -> Diff a -> a -> m Bool

-- | Reporting function (default = no report)
[roReport] :: RunOpts m a -> a -> m ()

-- | Number of batches to run (Nothing = run forever) (default = Nothing).
[roLimit] :: RunOpts m a -> Maybe Int

-- | Size of batching updates (1 = no batching) (default = 1)
[roBatch] :: RunOpts m a -> Int

-- | batches per report (Nothing = never report) (default = Just 1).
[roFreq] :: RunOpts m a -> Maybe Int

-- | Map over the underlying monad of a <a>RunOpts</a>.
hoistRunOpts :: (forall x. m x -> n x) -> RunOpts m a -> RunOpts n a

-- | Options for running an optimizer in a concurrent setting.
data ParallelOpts m a
PO :: Maybe Int -> Int -> (NonEmpty a -> m a) -> Bool -> ParallelOpts m a

-- | Number of threads (Nothing = max capacity) (default = Nothing)
[poThreads] :: ParallelOpts m a -> Maybe Int

-- | How many batches thread will process before regrouping (default =
--   1000)
[poSplit] :: ParallelOpts m a -> Int

-- | How to recombine a pool of updated results into a single result
--   (default = <tt><a>pure</a> <a>.</a> <a>mean</a></tt>)
[poCombine] :: ParallelOpts m a -> NonEmpty a -> m a

-- | For conduit runners, whether or not conduit is in "pull-based" mode,
--   where optimization doesn't happen until requested downstream. This is
--   ignored if not running via conduit (default = True)
[poPull] :: ParallelOpts m a -> Bool

-- | Map over the underlying monad of a <a>ParallelOpts</a>.
hoistParallelOpts :: (forall x. m x -> n x) -> ParallelOpts m a -> ParallelOpts n a

-- | Run an optimizer on some input, given a monadic action to produce each
--   new sample. When the action produces <a>Nothing</a>, the running
--   immediately terminates even if the stop condition has not yet been
--   met.
opto :: Monad m => RunOpts m a -> m (Maybe r) -> a -> Opto m v r a -> m a

-- | A version of <a>opto</a> that also returns an updated optimizer state
--   that can be resumed.
opto' :: Monad m => RunOpts m a -> m (Maybe r) -> a -> Opto m v r a -> m (a, Opto m v r a)

-- | A version of <a>optoNonSampling</a> that also returns an updated
--   optimizer state that can be resumed.
optoNonSampling' :: Monad m => RunOpts m a -> a -> Opto m v () a -> m (a, Opto m v () a)

-- | Run a non-sampling optimizer on some input until the stop condition is
--   met.
optoNonSampling :: Monad m => RunOpts m a -> a -> Opto m v () a -> m a

-- | Given an optimizer and some initial value, produce a <a>ConduitT</a>
--   that takes in samples and outputs each successively optimized versions
--   of the value. This essentially is a convenient wrapper over
--   <a>opto</a>.
--   
--   To get the <i>final</i> optimized result after a stream has
--   terminated, compose this with a sink like <a>last</a>.
--   
--   <pre>
--   <a>optoConduit</a> ro x0 o .| <a>last</a>
--     :: ConduitT r o m (Maybe a)
--   
--   <a>optoConduit</a> ro x0 o .| <a>lastDef</a> x0
--     :: ConduitT r o m a
--   </pre>
optoConduit :: Monad m => RunOpts m a -> a -> Opto (ConduitT r a m) v r a -> ConduitT r a m ()

-- | A version of <a>optoConduit</a> that also returns an updated optimizer
--   state that can be resumed.
optoConduit' :: Monad m => RunOpts m a -> a -> Opto (ConduitT r a m) v r a -> ConduitT r a m (Opto (ConduitT r a m) v r a)

-- | Convenient wrapper over <a>opto</a> to allow consumption over a list
--   of samples.
optoFold :: Monad m => RunOpts m a -> a -> Opto (StateT [r] m) v r a -> [r] -> m (a, [r])

-- | A version of <a>optoFold'</a> that also returns an updated optimizer
--   state that can be resumed.
optoFold' :: Monad m => RunOpts m a -> a -> Opto (StateT [r] m) v r a -> [r] -> m (a, [r], Opto (StateT [r] m) v r a)

-- | Run an optimizer in parallel on multiple threads on some value, given
--   a (thread-safe) monadic action to produce each new sample.
--   
--   It does this by repeatedly:
--   
--   <ol>
--   <li>Splitting into multiple threads (based on <a>poThreads</a>)</li>
--   <li>Running <a>opto</a> (single-threaded optimiztion) on each thread,
--   independently, from the same initial value.</li>
--   <li>After <a>poSplit</a> items have been processed, all threads wait
--   on each other to stop. After each thread is done, each thread's
--   optimized value is then aggregated using <a>poCombine</a> (by default,
--   it takes the mean).</li>
--   <li>This new optimized combined value is then used to begin the cycle
--   again.</li>
--   </ol>
--   
--   When action produces <a>Nothing</a> for <i>all</i> threads, the
--   running immediately terminates on all threads and returns even if the
--   stop condition has not yet been met. If the stop condition is met, the
--   value given to the stop condition will be used as the final result,
--   ignoring all other thread pools.
optoPar :: forall m v r a. MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> m (Maybe r) -> a -> Opto m v r a -> m a

-- | A version of <a>optoPar</a> that performs a batch fetch for each
--   thread's entire sample pool <i>before</i> beginning parallel
--   optimization. This can be useful if the sampling is faster in batch
--   amounts.
optoParChunk :: forall m v r a. MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> (Int -> m [r]) -> a -> Opto (StateT [r] m) v r a -> m a

-- | Run a non-sampling optimizer in parallel on multiple threads on some
--   value until the stop condition is met.
--   
--   See <a>optoPar</a> for a detailed description of how parallel
--   optimization is implemented.
optoParNonSampling :: MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> a -> Opto m v () a -> m a

-- | Given an optimizer, some initial value, and a conduit <i>source</i>,
--   returns a conduit sorce that outputs succesively optimized versions of
--   the value after each thread recombination, where each version is
--   optimized using parallel multi-threaded optimization.
--   
--   See <a>optoPar</a> for a detailed description on how parallel
--   optimization is implemented.
--   
--   Note that, unlike <a>optoConduit</a>, which is a conduit, this is a
--   conduit (source) <i>transformer</i>. It takes a source outputting
--   <i>samples</i> and returns a <i>new</i> source of <i>optimized
--   values</i>.
--   
--   A value is emitted after every thread recombination/call of
--   <a>poCombine</a>.
optoConduitPar :: forall m v r a. MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> a -> Opto m v r a -> ConduitT () r m () -> ConduitT () a m ()

-- | A version of <a>optoConduitPar</a> that performs a batch fetch from
--   the input source for each thread's entire sample pool <i>before</i>
--   beginning parallel optimization. This can be useful if the source can
--   produce values faster in batch amounts.
optoConduitParChunk :: forall m v r a. MonadUnliftIO m => RunOpts m a -> ParallelOpts m a -> a -> Opto (StateT [r] m) v r a -> ConduitT () r m () -> ConduitT () a m ()

-- | The mean of the values in a non-empty container.
mean :: (Foldable1 t, Fractional a) => t a -> a
instance (GHC.Num.Num a, GHC.Num.Num b) => GHC.Base.Semigroup (Numeric.Opto.Run.Sum2 a b)
instance (GHC.Num.Num a, GHC.Num.Num b) => GHC.Base.Monoid (Numeric.Opto.Run.Sum2 a b)
instance (GHC.Base.Applicative m, GHC.Real.Fractional a) => Data.Default.Class.Default (Numeric.Opto.Run.ParallelOpts m a)
instance GHC.Base.Functor m => Data.Functor.Invariant.Invariant (Numeric.Opto.Run.ParallelOpts m)
instance GHC.Base.Applicative m => Data.Default.Class.Default (Numeric.Opto.Run.RunOpts m a)
instance Data.Functor.Contravariant.Contravariant (Numeric.Opto.Run.RunOpts m)
instance Data.Functor.Invariant.Invariant (Numeric.Opto.Run.RunOpts m)


-- | Defining various numeric optimizers. Most of these implemtations are
--   taken directly from
--   <a>http://ruder.io/optimizing-gradient-descent/</a>
module Numeric.Opto.Optimizer

-- | Steepest descent, acording to some learning rate. The simplest
--   optimizer.
steepestDescent :: LinearInPlace m v c a => c -> Grad m r a -> Opto m v r a

-- | Hyperparameter for <a>momentum</a>
newtype Momentum c
Momentum :: c -> Momentum c
[momentumDecay] :: Momentum c -> c

-- | Steepest descent with momentum.
momentum :: forall m v r a c. (PrimMonad m, LinearInPlace m v c a) => Momentum c -> c -> Grad m r a -> Opto m v r a

-- | Hyperparameter for <a>nesterov</a>
newtype Nesterov c
Nesterov :: c -> Nesterov c
[nesterovDecay] :: Nesterov c -> c

-- | Nesterov accelerated gradient (NAG)
nesterov :: forall m v r a c. (PrimMonad m, LinearInPlace m v c a) => Nesterov c -> c -> Grad m r a -> Opto m v r a

-- | Hyperparameters for <a>adam</a>
data Adam c
Adam :: !c -> !c -> !c -> !c -> Adam c
[adamStep] :: Adam c -> !c
[adamDecay1] :: Adam c -> !c
[adamDecay2] :: Adam c -> !c
[adamEpsilon] :: Adam c -> !c

-- | Adaptive Moment Estimation
adam :: forall m v r a c. (RealFloat c, Floating a, LinearInPlace m v c a, PrimMonad m) => Adam c -> Grad m r a -> Opto m v r a

-- | Hyperparameters for <a>adaMax</a>
data AdaMax c
AdaMax :: !c -> !c -> !c -> !c -> AdaMax c
[adaMaxStep] :: AdaMax c -> !c
[adaMaxDecay1] :: AdaMax c -> !c
[adaMaxDecay2] :: AdaMax c -> !c
[adaMaxEpsilon] :: AdaMax c -> !c

-- | Adam variation (Kingma and Ba, 2015)
adaMax :: forall m v r a c. (RealFloat c, Metric c a, LinearInPlace m v c a, PrimMonad m) => AdaMax c -> Grad m r a -> Opto m v r a
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.AdaMax c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.AdaMax c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.Adam c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.Adam c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.Nesterov c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.Nesterov c)
instance GHC.Classes.Eq c => GHC.Classes.Eq (Numeric.Opto.Optimizer.Momentum c)
instance GHC.Show.Show c => GHC.Show.Show (Numeric.Opto.Optimizer.Momentum c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.AdaMax c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.Adam c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.Nesterov c)
instance GHC.Real.Fractional c => Data.Default.Class.Default (Numeric.Opto.Optimizer.Momentum c)


-- | Generate gradients usable with <a>Numeric.Opto</a> using the
--   <i>backprop</i> library.
module Numeric.Opto.Backprop

-- | Turn a simple <tt>a -&gt; b</tt> function into a <tt><a>Grad</a> m
--   a</tt>.
bpGrad :: (Monad m, Backprop a, Backprop b) => (forall s. Reifies s W => BVar s a -> BVar s b) -> Grad m r a

-- | Turn a <tt>a -&gt; b</tt> function parameterized on <tt>r</tt> into a
--   <tt><a>Grad</a> m a</tt>.
bpGradSample :: (Backprop a, Backprop b, Applicative m) => (forall s. Reifies s W => r -> BVar s a -> BVar s b) -> Grad m r a


-- | Main library entrypoint; essentially re-exports all of the submodules
--   to provide full functionality with a single import.
module Numeric.Opto
